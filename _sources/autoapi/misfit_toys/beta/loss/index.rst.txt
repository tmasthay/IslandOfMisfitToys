misfit_toys.beta.loss
=====================

.. py:module:: misfit_toys.beta.loss

.. autoapi-nested-parse::

   Centralized loss functions for use in misfit_toys.examples.hydra.main.
   This module also contains decorators to wrap around other loss functions.
   For example, see tik_reg, which can add Tikhonov regularization to any loss function.



Functions
---------

.. autoapisummary::

   misfit_toys.beta.loss.linear_combo
   misfit_toys.beta.loss.scurry
   misfit_toys.beta.loss.lin_decrease
   misfit_toys.beta.loss.tik_reg
   misfit_toys.beta.loss.w2
   misfit_toys.beta.loss.mse
   misfit_toys.beta.loss.w2_reg
   misfit_toys.beta.loss.w2_trunc
   misfit_toys.beta.loss.pdf_match
   misfit_toys.beta.loss.cdf_match
   misfit_toys.beta.loss.quantile_match
   misfit_toys.beta.loss.sobolev
   misfit_toys.beta.loss.huber
   misfit_toys.beta.loss.l1
   misfit_toys.beta.loss.l1_double
   misfit_toys.beta.loss.transform_loss
   misfit_toys.beta.loss.softplus
   misfit_toys.beta.loss.identity
   misfit_toys.beta.loss.inverse


Module Contents
---------------

.. py:function:: linear_combo(*, losses, weights=None)

   Computes a linear combination of multiple loss functions.

   :param losses: A list of loss functions.
   :type losses: list
   :param weights: A list of weights for each loss function. If not provided, equal weights are used.
   :type weights: list, optional

   :returns: A function that takes an input `x` and returns the linear combination of the loss functions.
   :rtype: function

   :raises AssertionError: If the number of losses is not equal to the number of weights.

   .. rubric:: Example

   >>> loss1 = lambda x: x**2
   >>> loss2 = lambda x: abs(x)
   >>> combined_loss = linear_combo(losses=[loss1, loss2], weights=[0.3, 0.7])
   >>> combined_loss(2)
   1.4


.. py:function:: scurry(**dec_kwargs)

   A decorator factory that allows passing keyword arguments to the decorated function.

   :param dec_kwargs: Keyword arguments to be passed to the decorated function.

   :returns: A decorator that adds the specified keyword arguments to the decorated function.


.. py:function:: lin_decrease(*, _min=1e-16, _max=1.0, max_calls)

   Returns a helper function that implements linear decrease.

   The helper function calculates a value between `_min` and `_max` based on the number of calls made to it.
   The value decreases linearly from `_max` to `_min` over `max_calls` number of calls.

   :param _min: The minimum value. Defaults to 1.0e-16.
   :type _min: float, optional
   :param _max: The maximum value. Defaults to 1.0.
   :type _max: float, optional
   :param max_calls: The maximum number of calls after which the value reaches `_min`.
   :type max_calls: int

   :returns: A helper function that calculates the linearly decreasing value.
   :rtype: function


.. py:function:: tik_reg(f, *, model_params, base_loss, weights, penalty=None, reg_sched=None)

   Applies Tikhonov regularization to a loss function.

   :param f: The loss function to be regularized.
   :param model_params: The parameters of the model.
   :param base_loss: The base loss function.
   :param weights: The weights for the misfit term and regularization term.
   :param penalty: The penalty factor for the regularization term (optional).
   :param reg_sched: The regularization schedule (optional).

   :returns: A helper function that applies Tikhonov regularization to the loss function.


.. py:function:: w2(f, *, renorm, x, p, tol=0.0001, max_iters=20, eps=0.0001)

   Calculates the W2 Wasserstein distance between the input function `f` and a target distribution.

   :param f: The input function.
   :param renorm: A function used to renormalize the input function.
   :param x: The input values.
   :param p: The quantile value.
   :param tol: The tolerance value for convergence (default: 1.0e-04).
   :param max_iters: The maximum number of iterations for convergence (default: 20).
   :param eps: A small value added to the input function to avoid division by zero (default: 1.0e-04).

   :returns: A function that calculates the W2 Wasserstein distance between the input function `f` and a target distribution.


.. py:function:: mse(f)

   Calculates the mean squared error (MSE) loss between two tensors.

   :param f: The first tensor.
   :type f: torch.Tensor

   :returns: A helper function that takes in another tensor and calculates the MSE loss.
   :rtype: function


.. py:function:: w2_reg(f, *, renorm, x, p, scale, tol=0.0001, max_iters=20)

   Computes the Wasserstein-2 regularization loss.

   :param f: The function to be regularized.
   :type f: callable
   :param renorm: The renormalization function.
   :type renorm: callable
   :param x: The input tensor.
   :type x: torch.Tensor
   :param p: The quantile value.
   :type p: float
   :param scale: The scaling factor for the regularization term.
   :type scale: float
   :param tol: The tolerance value for convergence. Defaults to 1.0e-04.
   :type tol: float, optional
   :param max_iters: The maximum number of iterations. Defaults to 20.
   :type max_iters: int, optional

   :returns: The helper function that computes the regularization loss.
   :rtype: callable


.. py:function:: w2_trunc(f, *, renorm, x, p, tol=0.0001, max_iters=20, eps=0.0, p_cutoff=0.001)

   Calculates the truncated Wasserstein-2 distance between two probability distributions.

   :param f: The function representing the probability distribution.
   :type f: callable
   :param renorm: The function used to renormalize the probability distribution.
   :type renorm: callable
   :param x: The input tensor.
   :type x: torch.Tensor
   :param p: The target probability distribution.
   :type p: torch.Tensor
   :param tol: The tolerance for convergence. Defaults to 1.0e-04.
   :type tol: float, optional
   :param max_iters: The maximum number of iterations. Defaults to 20.
   :type max_iters: int, optional
   :param eps: The epsilon value. Defaults to 0.0.
   :type eps: float, optional
   :param p_cutoff: The cutoff value for p. Defaults to 1.0e-03.
   :type p_cutoff: float, optional

   :returns: The helper function used for calculating the loss.
   :rtype: callable


.. py:function:: pdf_match(f, *, renorm, x)

   Returns a helper function that calculates the mean squared error (MSE) loss between two probability density functions (PDFs).

   :param f: The first PDF function.
   :param renorm: A function used to renormalize the PDFs.
   :param x: The input value for the PDFs.

   :returns: A helper function that takes in a second PDF function and returns the MSE loss between the renormalized PDFs of f and g, as well as an intermediate history object.


.. py:function:: cdf_match(f, *, renorm: Callable[[torch.Tensor, torch.Tensor], torch.Tensor], x: torch.Tensor)

   Compute the CDF match loss.

   :param f: The input function.
   :param renorm: A function that normalizes the input function.
   :param x: The input tensor.

   :returns: A helper function that computes the CDF match loss and returns the loss value and intermediate history.


.. py:function:: quantile_match(f, *, renorm: Callable[[torch.Tensor, torch.Tensor], torch.Tensor], x: torch.Tensor, p: torch.Tensor)

   Quantile matching loss function.

   :param f: The input function.
   :param renorm: A function that performs renormalization.
   :param x: The input tensor.
   :param p: The quantile tensor.

   :returns: A helper function that calculates the loss and returns the result along with the integration history.


.. py:function:: sobolev(f, *, scale, x)

   Calculates the Sobolev loss between two functions.

   :param f: The target function.
   :type f: torch.Tensor
   :param scale: The scale parameter for the Sobolev kernel.
   :type scale: float
   :param x: The input values.
   :type x: torch.Tensor

   :returns: A tuple containing the Sobolev loss and the integration history.
   :rtype: tuple


.. py:function:: huber(f, *, delta)

   Computes the Huber loss between the input `f` and a target value `g`.

   :param f: The input tensor.
   :type f: torch.Tensor
   :param delta: The threshold value for the Huber loss.
   :type delta: float

   :returns: The computed Huber loss.
   :rtype: torch.Tensor


.. py:function:: l1(f)

   Calculates the L1 loss between two tensors.

   :param f: The first tensor.
   :type f: Tensor

   :returns: A function that takes in another tensor and returns the L1 loss between the two tensors.
   :rtype: Callable


.. py:function:: l1_double(f, g)

   Calculates the mean absolute difference between two tensors.

   :param f: The first tensor.
   :type f: Tensor
   :param g: The second tensor.
   :type g: Tensor

   :returns: The mean absolute difference between f and g.
   :rtype: Tensor


.. py:function:: transform_loss(f, *, loss, transform)

   Applies a transformation function to the input functions and calculates the loss between the transformed functions.

   :param f: The input function.
   :param loss: The loss function used to calculate the loss between the transformed functions.
   :param transform: The transformation function applied to the input functions.

   :returns: A helper function that takes another function as input, applies the same transformation, and calculates the loss between the transformed functions.


.. py:function:: softplus(*, scale=1.0, t)

   Compute the softplus function.

   :param scale: Scaling factor for the input. Defaults to 1.0.
   :type scale: float, optional
   :param t: Input tensor.
   :type t: torch.Tensor

   :returns: The cumulative distribution function (CDF) of the softplus function.
   :rtype: torch.Tensor


.. py:function:: identity(f)

   Returns the input function `f` unchanged.

   :param f: The input function.

   :returns: The input function `f`.


.. py:function:: inverse(f, x)

   Computes the inverse of a function using natural cubic splines.

   :param f: The function to compute the inverse of.
   :param x: The input value.

   :returns: The inverse of the function evaluated at the given input value.


